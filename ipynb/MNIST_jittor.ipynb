{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfd990d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T14:14:28.025328Z",
     "start_time": "2022-04-07T14:14:26.717418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0407 22:14:26.749072 96 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:26.778917 96 compiler.py:951] Jittor(1.3.1.56) src: /home/hx-gpu3/anaconda3/envs/mmlab/lib/python3.9/site-packages/jittor\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:26.781350 96 compiler.py:952] g++ at /usr/bin/g++(7.5.0)\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:26.781901 96 compiler.py:953] cache_path: /home/hx-gpu3/.cache/jittor/jt1.3.1/g++7.5.0/py3.9.10/Linux-5.4.0-99x95/IntelRCoreTMi9x92/default\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:26.784405 96 compiler.py:896] Found nvcc(11.0.194) at /usr/local/cuda-11.0/bin/nvcc\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:26.818760 96 __init__.py:411] Found gdb(8.1.0) at /usr/bin/gdb.\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:26.821951 96 __init__.py:411] Found addr2line(2.30) at /usr/bin/addr2line.\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:26.879117 96 compiler.py:1006] cuda key:cu11.0.194_sm_86\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:26.976861 96 __init__.py:227] Total mem: 62.74GB, using 16 procs for compiling.\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.043461 96 jit_compiler.cc:28] Load cc_path: /usr/bin/g++\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.044020 96 jit_compiler.cc:31] Load nvcc_path: /usr/local/cuda-11.0/bin/nvcc\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.092084 96 init.cc:62] Found cuda archs: [86,]\u001b[m\n",
      "\u001b[38;5;3m[w 0407 22:14:27.139663 96 compiler.py:1351] CUDA arch(86)>80 will be backward-compatible\u001b[m\n",
      "gcc: error: unrecognized command line option ‘--showme:version’; did you mean ‘--no-version’?\n",
      "\u001b[38;5;2m[i 0407 22:14:27.154095 96 compile_extern.py:516] mpicc not found, distribution disabled.\u001b[m\n",
      "\u001b[38;5;3m[w 0407 22:14:27.168702 96 compile_extern.py:200] CUDA related path found in LD_LIBRARY_PATH or PATH(['', '/usr/local/cuda-11.1/lib64', '/home/hx-gpu3/anaconda3/envs/mmlab/bin', '/home/hx-gpu3/anaconda3/condabin', '/usr/local/sbin', '/usr/local/bin', '/usr/sbin', '/usr/bin', '/sbin', '/bin', '/usr/games', '/usr/local/games', '/snap/bin', '/usr/local/cuda-11.1/bin', '/home/hx-gpu3/.myscripts']), This path may cause jittor found the wrong libs, please unset LD_LIBRARY_PATH and remove cuda lib path in Path. \u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.182243 96 compile_extern.py:30] found /usr/local/cuda-11.0/include/cublas.h\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.186680 96 compile_extern.py:30] found /usr/local/cuda-11.0/lib64/libcublas.so\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.187217 96 compile_extern.py:30] found /usr/local/cuda-11.0/lib64/libcublasLt.so.11\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.550385 96 compile_extern.py:30] found /usr/local/cuda-11.0/include/cudnn.h\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.567598 96 compile_extern.py:30] found /usr/local/cuda-11.0/lib64/libcudnn.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.568339 96 compile_extern.py:30] found /usr/local/cuda-11.0/lib64/libcudnn_ops_infer.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.569994 96 compile_extern.py:30] found /usr/local/cuda-11.0/lib64/libcudnn_ops_train.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.570503 96 compile_extern.py:30] found /usr/local/cuda-11.0/lib64/libcudnn_cnn_infer.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.661484 96 compile_extern.py:30] found /usr/local/cuda-11.0/lib64/libcudnn_cnn_train.so.8\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:27.981568 96 compile_extern.py:30] found /usr/local/cuda-11.0/include/curand.h\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:28.002143 96 compile_extern.py:30] found /usr/local/cuda-11.0/lib64/libcurand.so\u001b[m\n",
      "\u001b[38;5;2m[i 0407 22:14:28.023488 96 cuda_flags.cc:32] CUDA enabled.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# classification mnist example \n",
    "import  os\n",
    "os.environ['nvcc_path']='/usr/local/cuda-11.0/bin/nvcc'\n",
    "import jittor as jt  # 将 jittor 引入\n",
    "from jittor import nn, Module  # 引入相关的模块\n",
    "import numpy as np\n",
    "import math \n",
    "from jittor import init\n",
    "if jt.has_cuda:\n",
    "    jt.flags.use_cuda = 1 # jt.flags.use_cuda 表示是否使用 gpu 训练。\n",
    "# 如果 jt.flags.use_cuda=1，表示使用GPU训练 如果 jt.flags.use_cuda = 0 表示使用 CPU\n",
    "from jittor.dataset.mnist import MNIST \n",
    "#由于 MNIST 是一个常见的数据集，其数据载入已经被封装进 jittor 所以可以直接调用。\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163be0a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T14:14:28.032412Z",
     "start_time": "2022-04-07T14:14:28.026401Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from PIL import Image\n",
    "from jittor.dataset import Dataset\n",
    "from jittor_utils.misc import download_url_to_local\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    def __init__(self, data_root=\"./mnist_data/\", train=True ,download=True, batch_size=1, shuffle=False):\n",
    "        # if you want to test resnet etc you should set input_channel = 3, because the net set 3 as the input dimensions\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.is_train = train\n",
    "        if download == True:\n",
    "            self.download_url()\n",
    "\n",
    "        filesname = [\n",
    "                \"train-images-idx3-ubyte.gz\",\n",
    "                \"t10k-images-idx3-ubyte.gz\",\n",
    "                \"train-labels-idx1-ubyte.gz\",\n",
    "                \"t10k-labels-idx1-ubyte.gz\"\n",
    "        ]\n",
    "        self.mnist = {}\n",
    "        if self.is_train:\n",
    "            with gzip.open(data_root + filesname[0], 'rb') as f:\n",
    "                self.mnist[\"images\"] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28, 28)\n",
    "            with gzip.open(data_root + filesname[2], 'rb') as f:\n",
    "                self.mnist[\"labels\"] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        else:\n",
    "            with gzip.open(data_root + filesname[1], 'rb') as f:\n",
    "                self.mnist[\"images\"] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28, 28)\n",
    "            with gzip.open(data_root + filesname[3], 'rb') as f:\n",
    "                self.mnist[\"labels\"] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        assert(self.mnist[\"images\"].shape[0] == self.mnist[\"labels\"].shape[0])\n",
    "        self.total_len = self.mnist[\"images\"].shape[0]\n",
    "        # this function must be called\n",
    "        self.set_attrs(batch_size = self.batch_size, total_len=self.total_len, shuffle= self.shuffle, num_workers=4)\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.fromarray (self.mnist['images'][index]) \n",
    "        img = np.array (img)\n",
    "        img = img[np.newaxis, :]\n",
    "        return np.array((img / 255.0), dtype = np.float32), self.mnist['labels'][index]\n",
    "\n",
    "    def download_url(self):\n",
    "        resources = [\n",
    "            (\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\", \"f68b3c2dcbeaaa9fbdd348bbdeb94873\"),\n",
    "            (\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\", \"d53e105ee54ea40749a09fcbcd1e9432\"),\n",
    "            (\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\", \"9fb629c4189551a2d022fa330f9573f3\"),\n",
    "            (\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\", \"ec29112dd5afa0611ce80d1b7f02629c\")\n",
    "        ]\n",
    "\n",
    "        for url, md5 in resources:\n",
    "            filename = url.rpartition('/')[2]\n",
    "            download_url_to_local(url, filename, self.data_root, md5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8168975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T14:14:28.039572Z",
     "start_time": "2022-04-07T14:14:28.033501Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model (Module):\n",
    "    def __init__ (self):\n",
    "        super (Model, self).__init__()\n",
    "\n",
    "        self.relu = nn.Relu()\n",
    "        self.fc1 = nn.Linear (28*28, 512)\n",
    "        self.fc2 = nn.Linear (512, 10)\n",
    "    def execute (self, x) : \n",
    "        # it's simliar to forward function in Pytorch \n",
    "        x=jt.reshape(x,[x.shape[0],-1])\n",
    "        \n",
    "        x = self.fc1 (x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2 (x)\n",
    "#         x= nn.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99747bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T14:14:30.169419Z",
     "start_time": "2022-04-07T14:14:28.040321Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1875 (0%)]\tLoss: 2.307442\n",
      "Train Epoch: 1 [500/1875 (27%)]\tLoss: 0.208463\n",
      "Train Epoch: 1 [1000/1875 (53%)]\tLoss: 0.178393\n",
      "Train Epoch: 1 [1500/1875 (80%)]\tLoss: 0.073652\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, optimizer, epoch, losses, losses_idx):\n",
    "    model.train()\n",
    "    lens = len(train_loader)\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = nn.cross_entropy_loss(outputs, targets)\n",
    "        optimizer.step (loss)\n",
    "        losses.append(loss.numpy()[0])\n",
    "#         losses_idx.append(epoch * lens + batch_idx)\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(train_loader) ,\n",
    "                100. * batch_idx / len(train_loader), loss.numpy()[0]))\n",
    "\n",
    "def val(model, val_loader, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        batch_size = inputs.shape[0]\n",
    "        outputs = model(inputs)\n",
    "        pred = np.argmax(outputs.numpy(), axis=1)\n",
    "        acc = np.sum(targets.numpy()==pred)\n",
    "        total_acc += acc\n",
    "        total_num += batch_size\n",
    "        acc = acc / batch_size\n",
    "        \n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(f'Test Epoch: {epoch} [{batch_idx}/{len(val_loader)}]\\tAcc: {acc:.6f}')    \t\n",
    "            print('Test Acc =', total_acc / total_num)\n",
    "    \n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "epochs = 20\n",
    "losses = []\n",
    "losses_idx = []\n",
    "train_loader = MNIST(train=True, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_loader = MNIST(train=False, batch_size=1, shuffle=False)\n",
    "\n",
    "model = Model ()\n",
    "optimizer = nn.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "train(model, train_loader, optimizer, 1, losses, losses_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e0918d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T14:14:55.049076Z",
     "start_time": "2022-04-07T14:14:30.170347Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1875 (0%)]\tLoss: 0.122890\n",
      "Train Epoch: 0 [500/1875 (27%)]\tLoss: 0.112210\n",
      "Train Epoch: 0 [1000/1875 (53%)]\tLoss: 0.046900\n",
      "Train Epoch: 0 [1500/1875 (80%)]\tLoss: 0.221125\n",
      "Train Epoch: 1 [0/1875 (0%)]\tLoss: 0.079623\n",
      "Train Epoch: 1 [500/1875 (27%)]\tLoss: 0.133670\n",
      "Train Epoch: 1 [1000/1875 (53%)]\tLoss: 0.053771\n",
      "Train Epoch: 1 [1500/1875 (80%)]\tLoss: 0.190562\n",
      "Train Epoch: 2 [0/1875 (0%)]\tLoss: 0.004243\n",
      "Train Epoch: 2 [500/1875 (27%)]\tLoss: 0.008934\n",
      "Train Epoch: 2 [1000/1875 (53%)]\tLoss: 0.017167\n",
      "Train Epoch: 2 [1500/1875 (80%)]\tLoss: 0.062817\n",
      "Train Epoch: 3 [0/1875 (0%)]\tLoss: 0.120871\n",
      "Train Epoch: 3 [500/1875 (27%)]\tLoss: 0.010004\n",
      "Train Epoch: 3 [1000/1875 (53%)]\tLoss: 0.002913\n",
      "Train Epoch: 3 [1500/1875 (80%)]\tLoss: 0.006491\n",
      "Train Epoch: 4 [0/1875 (0%)]\tLoss: 0.030921\n",
      "Train Epoch: 4 [500/1875 (27%)]\tLoss: 0.010778\n",
      "Train Epoch: 4 [1000/1875 (53%)]\tLoss: 0.009297\n",
      "Train Epoch: 4 [1500/1875 (80%)]\tLoss: 0.125535\n",
      "Train Epoch: 5 [0/1875 (0%)]\tLoss: 0.048438\n",
      "Train Epoch: 5 [500/1875 (27%)]\tLoss: 0.004680\n",
      "Train Epoch: 5 [1000/1875 (53%)]\tLoss: 0.000276\n",
      "Train Epoch: 5 [1500/1875 (80%)]\tLoss: 0.128771\n",
      "Train Epoch: 6 [0/1875 (0%)]\tLoss: 0.006208\n",
      "Train Epoch: 6 [500/1875 (27%)]\tLoss: 0.000893\n",
      "Train Epoch: 6 [1000/1875 (53%)]\tLoss: 0.001922\n",
      "Train Epoch: 6 [1500/1875 (80%)]\tLoss: 0.000939\n",
      "Train Epoch: 7 [0/1875 (0%)]\tLoss: 0.001965\n",
      "Train Epoch: 7 [500/1875 (27%)]\tLoss: 0.001532\n",
      "Train Epoch: 7 [1000/1875 (53%)]\tLoss: 0.004513\n",
      "Train Epoch: 7 [1500/1875 (80%)]\tLoss: 0.000102\n",
      "Train Epoch: 8 [0/1875 (0%)]\tLoss: 0.005605\n",
      "Train Epoch: 8 [500/1875 (27%)]\tLoss: 0.000301\n",
      "Train Epoch: 8 [1000/1875 (53%)]\tLoss: 0.030343\n",
      "Train Epoch: 8 [1500/1875 (80%)]\tLoss: 0.046863\n",
      "Train Epoch: 9 [0/1875 (0%)]\tLoss: 0.048261\n",
      "Train Epoch: 9 [500/1875 (27%)]\tLoss: 0.000172\n",
      "Train Epoch: 9 [1000/1875 (53%)]\tLoss: 0.000765\n",
      "Train Epoch: 9 [1500/1875 (80%)]\tLoss: 0.025166\n",
      "Train Epoch: 10 [0/1875 (0%)]\tLoss: 0.050813\n",
      "Train Epoch: 10 [500/1875 (27%)]\tLoss: 0.018115\n",
      "Train Epoch: 10 [1000/1875 (53%)]\tLoss: 0.001482\n",
      "Train Epoch: 10 [1500/1875 (80%)]\tLoss: 0.001620\n",
      "Train Epoch: 11 [0/1875 (0%)]\tLoss: 0.001126\n",
      "Train Epoch: 11 [500/1875 (27%)]\tLoss: 0.000065\n",
      "Train Epoch: 11 [1000/1875 (53%)]\tLoss: 0.000015\n",
      "Train Epoch: 11 [1500/1875 (80%)]\tLoss: 0.000246\n",
      "Train Epoch: 12 [0/1875 (0%)]\tLoss: 0.000259\n",
      "Train Epoch: 12 [500/1875 (27%)]\tLoss: 0.000009\n",
      "Train Epoch: 12 [1000/1875 (53%)]\tLoss: 0.000079\n",
      "Train Epoch: 12 [1500/1875 (80%)]\tLoss: 0.166439\n",
      "Train Epoch: 13 [0/1875 (0%)]\tLoss: 0.003817\n",
      "Train Epoch: 13 [500/1875 (27%)]\tLoss: 0.007130\n",
      "Train Epoch: 13 [1000/1875 (53%)]\tLoss: 0.000122\n",
      "Train Epoch: 13 [1500/1875 (80%)]\tLoss: 0.000061\n",
      "Train Epoch: 14 [0/1875 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 14 [500/1875 (27%)]\tLoss: 0.000085\n",
      "Train Epoch: 14 [1000/1875 (53%)]\tLoss: 0.000460\n",
      "Train Epoch: 14 [1500/1875 (80%)]\tLoss: 0.000064\n",
      "Train Epoch: 15 [0/1875 (0%)]\tLoss: 0.002540\n",
      "Train Epoch: 15 [500/1875 (27%)]\tLoss: 0.008511\n",
      "Train Epoch: 15 [1000/1875 (53%)]\tLoss: 0.000083\n",
      "Train Epoch: 15 [1500/1875 (80%)]\tLoss: 0.000531\n",
      "Train Epoch: 16 [0/1875 (0%)]\tLoss: 0.000410\n",
      "Train Epoch: 16 [500/1875 (27%)]\tLoss: 0.004465\n",
      "Train Epoch: 16 [1000/1875 (53%)]\tLoss: 0.000081\n",
      "Train Epoch: 16 [1500/1875 (80%)]\tLoss: 0.000407\n",
      "Train Epoch: 17 [0/1875 (0%)]\tLoss: 0.001529\n",
      "Train Epoch: 17 [500/1875 (27%)]\tLoss: 0.000785\n",
      "Train Epoch: 17 [1000/1875 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 17 [1500/1875 (80%)]\tLoss: 0.000296\n",
      "Train Epoch: 18 [0/1875 (0%)]\tLoss: 0.000012\n",
      "Train Epoch: 18 [500/1875 (27%)]\tLoss: 0.000202\n",
      "Train Epoch: 18 [1000/1875 (53%)]\tLoss: 0.000007\n",
      "Train Epoch: 18 [1500/1875 (80%)]\tLoss: 0.010070\n",
      "Train Epoch: 19 [0/1875 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 19 [500/1875 (27%)]\tLoss: 0.000019\n",
      "Train Epoch: 19 [1000/1875 (53%)]\tLoss: 0.006054\n",
      "Train Epoch: 19 [1500/1875 (80%)]\tLoss: 0.000015\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(model, train_loader, optimizer, epoch, losses, losses_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
